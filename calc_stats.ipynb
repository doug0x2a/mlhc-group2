{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bootstrap_stats(y_true, y_prob1, y_prob2, B=10000):\n",
    "    results = {}\n",
    "    results['auc1'] = roc_auc_score(y_true, y_prob1)\n",
    "    results['auc2'] = roc_auc_score(y_true, y_prob2)\n",
    "    results['auc_diff'] = results['auc2'] - results['auc1']\n",
    "    aucs1 = []\n",
    "    aucs2 = []\n",
    "    auc_diffs = []\n",
    "    results['f1_score1'] = f1_score(y_true, y_prob1 > 0.5)\n",
    "    results['f1_score2'] = f1_score(y_true, y_prob2 > 0.5)\n",
    "    results['f1_diff'] = results['f1_score2'] - results['f1_score1']\n",
    "    f1_scores1 = []\n",
    "    f1_scores2 = []\n",
    "    f1_diffs = []\n",
    "\n",
    "    # Run the bootstrap\n",
    "    for _ in tqdm(range(B)):\n",
    "        idx = np.random.choice(range(len(y_true)), len(y_true), replace=True)\n",
    "        auc1 = roc_auc_score(y_true[idx], y_prob1[idx])\n",
    "        auc2 = roc_auc_score(y_true[idx], y_prob2[idx])\n",
    "        f1_1 = f1_score(y_true[idx], y_prob1[idx] > 0.5)\n",
    "        f1_2 = f1_score(y_true[idx], y_prob2[idx] > 0.5)\n",
    "        aucs1.append(auc1)\n",
    "        aucs2.append(auc2)\n",
    "        auc_diffs.append(auc2 - auc1)\n",
    "        f1_scores1.append(f1_1)\n",
    "        f1_scores2.append(f1_2)\n",
    "        f1_diffs.append(f1_2 - f1_1)\n",
    "    \n",
    "    # Compute confidence intervals\n",
    "    aucs1 = np.array(aucs1)\n",
    "    aucs2 = np.array(aucs2)\n",
    "    auc_diffs = np.array(auc_diffs)\n",
    "    results['auc_diff_ci'] = np.percentile(auc_diffs, [2.5, 97.5])\n",
    "    if results['auc2'] > results['auc1']:\n",
    "        results['auc_diff_p_value'] = (auc_diffs < 0).mean()\n",
    "    else:\n",
    "        results['auc_diff_p_value'] = (auc_diffs > 0).mean()\n",
    "    results['auc1_ci'] = np.percentile(aucs1, [2.5, 97.5])\n",
    "    results['auc2_ci'] = np.percentile(aucs2, [2.5, 97.5])\n",
    "    f1_scores1 = np.array(f1_scores1)\n",
    "    f1_scores2 = np.array(f1_scores2)\n",
    "    f1_diffs = np.array(f1_diffs)\n",
    "    results['f1_diff_ci'] = np.percentile(f1_diffs, [2.5, 97.5])\n",
    "    if results['f1_score2'] > results['f1_score1']:\n",
    "        results['f1_diff_p_value'] = (f1_diffs < 0).mean()\n",
    "    else:\n",
    "        results['f1_diff_p_value'] = (f1_diffs > 0).mean()\n",
    "    results['f1_score1_ci'] = np.percentile(f1_scores1, [2.5, 97.5])\n",
    "    results['f1_score2_ci'] = np.percentile(f1_scores2, [2.5, 97.5])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BRSet Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "brset_embed = pd.read_csv('embeddings.csv') # From Embeddings archive\n",
    "brset_split = pd.read_csv('split.csv') # generated from resplit_data.ipynb\n",
    "\n",
    "# Load the true values\n",
    "y_test = np.array(brset_embed[brset_split['split'] == 'test']['DR_2'])\n",
    "y_test_embed = np.array(brset_embed[brset_split['embeddings_split'] == 'test']['DR_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note Analysis - All columns vs Patient History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:37<00:00, 267.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'auc1': 0.9769691825582075,\n",
       " 'auc2': 0.8511669173024821,\n",
       " 'auc_diff': -0.12580226525572547,\n",
       " 'f1_score1': 0.8648648648648649,\n",
       " 'f1_score2': 0.3486842105263158,\n",
       " 'f1_diff': -0.5161806543385491,\n",
       " 'auc_diff_ci': array([-0.14976049, -0.1026009 ]),\n",
       " 'auc_diff_p_value': 0.0,\n",
       " 'auc1_ci': array([0.96567581, 0.98658303]),\n",
       " 'auc2_ci': array([0.82568875, 0.87555933]),\n",
       " 'f1_diff_ci': array([-0.59164667, -0.43990956]),\n",
       " 'f1_diff_p_value': 0.0,\n",
       " 'f1_score1_ci': array([0.82653061, 0.89908257]),\n",
       " 'f1_score2_ci': array([0.27857143, 0.41640425])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_all_columns_test_probs = np.load('probs/xgb_all_columns_test_probs.npy')\n",
    "xgb_pt_history_test_probs = np.load('probs/xgb_pt_history_test_probs.npy')\n",
    "\n",
    "results = bootstrap_stats(y_test, xgb_all_columns_test_probs, xgb_pt_history_test_probs, B=10000)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mit-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
